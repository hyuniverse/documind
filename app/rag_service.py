import os
from typing import Optional, Tuple, List, Any # ìƒˆë¡œ ì¶”ê°€ëœ íƒ€ì… íŒíŠ¸
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_core.document_loaders import BaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.vectorstores import VectorStore
from langchain_openai import ChatOpenAI
from langchain_community.docstore.document import Document
from langchain_openai import OpenAIEmbeddings
from langchain_core.runnables import RunnableMap, RunnablePassthrough, Runnable

# ===================================================
# 1. í™˜ê²½ ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜
# ===================================================

# Chroma DB íŒŒì¼ì´ ì €ì¥ë  ë¡œì»¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ì €ì¥ë˜ë„ë¡ í•˜ì—¬ git ê´€ë¦¬ ìš©ì´í•˜ë„ë¡ í•¨
PERSIST_DIRECTORY: str = "./chroma_db"
# RAG ì‹œìŠ¤í…œì— ì‚¬ìš©ë  ì›ë³¸ ë¬¸ì„œ ê²½ë¡œ
# TODO: ì‹¤ì œ ìš´ì˜ ì‹œ ë™ì ìœ¼ë¡œ ë³€ê²½ë  ìˆ˜ ìˆë„ë¡ ê°œì„  í•„ìš”
DOCUMENT_PATH: str = os.path.join(os.path.dirname(__file__), "sample.pdf")

# ì´ˆê¸°í™”ëœ ë²¡í„° DB ì¸ìŠ¤í„´ìŠ¤
# FastAPI ì„œë¹„ìŠ¤ ì „ì—­ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ ìºì‹œ ì—­í• ì„ ìˆ˜í–‰í•¨.
# Noneìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©°, initialize_rag_database() í˜¸ì¶œ í›„ Chroma ì¸ìŠ¤í„´ìŠ¤ë¡œ ì—…ë°ì´íŠ¸ë¨.
vectorstore: Optional[VectorStore] = None

qa_chain: Optional[Runnable] = None


# ===================================================
# 2. Utility Functions
# ===================================================   

def get_loader() -> BaseLoader:
    """
    ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ(DOCUMENT_PATH)ì— ë”°ë¼ ì ì ˆí•œ Langchain Loader ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    Returns:
        BaseLoader: íŒŒì¼ íƒ€ì…ì— ë§ëŠ” ë¡œë”(TextLoader or PyPDFLoader) ì¸ìŠ¤í„´ìŠ¤
    Raises:
        ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì¸ ê²½ìš°
        FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
    """
    if not os.path.exists(DOCUMENT_PATH):
        raise FileNotFoundError(f"ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DOCUMENT_PATH}")
    
    if DOCUMENT_PATH.endswith(".pdf"):
        return PyPDFLoader(DOCUMENT_PATH)
    elif DOCUMENT_PATH.endswith(".txt"):
        return TextLoader(DOCUMENT_PATH)
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

# ===================================================
# 3. RAG ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” í•¨ìˆ˜
# ===================================================

def initialize_RAG_database():
    global vectorstore

    # OpenAI API í‚¤ í™•ì¸
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    print(f"[RAG Service] OpenAI API í‚¤ í™•ì¸: {openai_api_key[:20]}..." if len(openai_api_key) > 20 else openai_api_key)

    # 1. DBê°€ ì´ë¯¸ ë¡œì»¬ì— ì¡´ì¬í•  ê²½ìš°
    #    ìƒˆë¡œ ì„ë² ë”©ì„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ë¡œë“œ
    if os.path.exists(PERSIST_DIRECTORY):
        print("[RAG Service] ê¸°ì¡´ Chroma DB ë¡œë“œ ì¤‘...")

        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)
        print("[RAG Service] Chroma DB ë¡œë“œ ì™„ë£Œ.")
        return
    
    # 2. DBê°€ ë¡œì»¬ì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.(Indexing Pipeline ì‹¤í–‰)
    print(f"[{DOCUMENT_PATH}] ë¬¸ì„œ ë¡œë“œ ë° DB ìƒì„± ì‹œì‘...")
    print(f"[RAG Service] ë¬¸ì„œ ê²½ë¡œ: {DOCUMENT_PATH}")
    print(f"[RAG Service] ë¬¸ì„œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(DOCUMENT_PATH)}")

    loader = get_loader()
    documents = loader.load()
    print(f"ì´ {len(documents)}ê°œì˜ LangChain Document ê°ì²´ ë¡œë“œ ì™„ë£Œ.")
    
    # ë¬¸ì„œ ë‚´ìš© ìƒ˜í”Œ ì¶œë ¥
    if documents:
        print(f"[RAG Service] ì²« ë²ˆì§¸ ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {documents[0].page_content[:200]}...")
        print(f"[RAG Service] ì²« ë²ˆì§¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°: {documents[0].metadata}")

    # ë¬¸ì„œ ë¶„í• (Chunking)
    # RecursiveCharacterTextSplitterëŠ” ë‹¤ì–‘í•œ êµ¬ë¶„ì(newline, space, punctuation ë“±)ë¥¼ í™œìš©í•˜ì—¬
    # í…ìŠ¤íŠ¸ê°€ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ìª¼ê°œì§€ë„ë¡ ì‹œë„í•©ë‹ˆë‹¤.
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200 # 200í† í° ì¤‘ë³µ ì„¤ì •ìœ¼ë¡œ ì²­í¬ ê°„ì˜ ë§¥ë½ ìœ ì§€
    )

    texts = text_splitter.split_documents(documents)
    print(f"ë¬¸ì„œ ë¶„í•  ì™„ë£Œ. ì´ {len(texts)}ê°œì˜ ì²­í¬ ìƒì„±ë¨.")
    
    # ì²­í¬ ë‚´ìš© ìƒ˜í”Œ ì¶œë ¥
    if texts:
        print(f"[RAG Service] ì²« ë²ˆì§¸ ì²­í¬ ë‚´ìš©: {texts[0].page_content[:200]}...")

    # ì„ë² ë”© ìƒì„± ë° ë²¡í„° DB ì €ì¥ (ì‹¤ì œ RAG ë°ì´í„° êµ¬ì¡°í™” ë‹¨ê³„)
    # OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©í•´ ê° ì²­í¬ì˜ ë²¡í„° í‘œí˜„ ìƒì„±(ë¹„ìš© ë°œìƒ ğŸ’¸)
    print("[RAG Service] ì„ë² ë”© ìƒì„± ì¤‘... (OpenAI API í˜¸ì¶œ)")
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(
        documents=texts,
        embedding=embeddings,
        persist_directory=PERSIST_DIRECTORY
    )

    print(f"âœ… [RAG Service] ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ [{PERSIST_DIRECTORY}]ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì €ì¥ëœ ë²¡í„° ìˆ˜ í™•ì¸
    try:
        # Chromaì˜ ìƒˆë¡œìš´ API ì‚¬ìš©
        collection_count = len(texts)  # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ì™€ ë™ì¼
        print(f"[RAG Service] ì €ì¥ëœ ë²¡í„° ìˆ˜: {collection_count}")
    except Exception as e:
        print(f"[RAG Service] ë²¡í„° ìˆ˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")


def get_retriever():
    global vectorstore

    if vectorstore is None:
        raise RuntimeError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RAG ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¨¼ì € ì´ˆê¸°í™”í•˜ì„¸ìš”.")

    return vectorstore.as_retriever(search_kwargs={"k": 3})

def get_qa_chain():
    """
    ì´ˆê¸°í™”ëœ vectorstoreë¥¼ ê¸°ë°˜ìœ¼ë¡œ RetrievalQA Chainì„ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        RetrievalQA: ì§ˆë¬¸ ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” Langchain ì²´ì¸ ì¸ìŠ¤í„´ìŠ¤
    Raises:
        RuntimeError: ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
    """
    global qa_chain, vectorstore

    if qa_chain is not None:
        return qa_chain
    
    if vectorstore is None:
        raise RuntimeError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RAG ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¨¼ì € ì´ˆê¸°í™”í•˜ì„¸ìš”.")
    
    # 1. LLM ì •ì˜ - OpenAIì˜ GPT-3.5 Turbo ëª¨ë¸ ì‚¬ìš©
    llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")

    # 2. Retriever ì •ì˜ - DBì—ì„œ ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
    # search_kwargs={"k": 3} ì€ ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 3ê°œë¥¼ ê²€ìƒ‰í•˜ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
    retriever = get_retriever()

    qa_chain = (
        RunnableMap({
            "context": retriever,
            "question": RunnablePassthrough() # queryëŠ” ê·¸ëŒ€ë¡œ LLMì— ì „ë‹¬
        })
        | (lambda x: f"ì§ˆë¬¸: {x['question']}\në¬¸ì„œ: {x['context']}\në‹µë³€:")  # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        | llm
    )
    return qa_chain


    

def answer_question(question: str) -> Tuple[str, List[Document]]:
    """
    ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‹œìŠ¤í…œì„ í†µí•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        question (str): ì‚¬ìš©ìë¡œë¶€í„° ë°›ì€ ì§ˆë¬¸ ë¬¸ìì—´

    Returns:
        Tuple[str, List[Document]]: ìƒì„±ëœ ë‹µë³€ ë¬¸ìì—´ê³¼ ê·¼ê±° ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    print(f"[RAG Service] ì§ˆë¬¸ ë°›ìŒ: {question}")
    
    # ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ í™•ì¸
    if vectorstore is None:
        print("[RAG Service] ì˜¤ë¥˜: vectorstoreê°€ Noneì…ë‹ˆë‹¤.")
        raise RuntimeError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ë¨¼ì € retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    try:
        retriever = get_retriever()
        sources = retriever.invoke(question)
        print(f"[RAG Service] ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(sources)}")
        
        if sources:
            for i, doc in enumerate(sources):
                print(f"[RAG Service] ë¬¸ì„œ {i+1}: {doc.page_content[:100]}...")
        else:
            print("[RAG Service] ê²½ê³ : ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            
    except Exception as e:
        print(f"[RAG Service] ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise
    
    # QA Chain ì‹¤í–‰
    try:
        qa = get_qa_chain()
        print("[RAG Service] QA Chain ì‹¤í–‰ ì¤‘...")
        result = qa.invoke(question)
        print(f"[RAG Service] LLM ì‘ë‹µ íƒ€ì…: {type(result)}")
        
        # AIMessageì˜ contentë¥¼ ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
        answer_text = str(result.content) if hasattr(result, 'content') else str(result)
        print(f"[RAG Service] ìµœì¢… ë‹µë³€: {answer_text[:200]}...")
        
        return answer_text, sources
        
    except Exception as e:
        print(f"[RAG Service] QA Chain ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise
